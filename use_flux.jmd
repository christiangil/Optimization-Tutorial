
```julia
include("create_jmds.jl")
```

1. Modify the use_flux notebook to start with a simple linear regression problem (fitting a noisy cubic with a n-degree polynomial?)
2. show how regularization can help
3. explain logistic regression
4. then use it to fit a small neural net (regularized and not) to convert the features to stability scores.

5. Break off the Tutorial notebook above "Training models!" to be a record of how we are making the dataset that people can look at. 
6. (or maybe lower):  If we want people to learn about XGB, I can clean up the lower section and write up a summary of their intro to boosted trees (https://xgboost.readthedocs.io/en/latest/tutorials/model.html). 

For 1-4, I'm thinking that we'd use the data that you generate with the notebook in step 2. split #1 into two tutorials.  One really simple one (e.g., fitting a polynomial to data that's generated as a polynomial plus noise), and then another one that's doing the same basic things, but using your more complicated dataset/features.

# Optimization Tutorial

blah blah

## Ordinary least squares

If you can formulate your model in a linear fashion (i.e. $y = X \beta + \epsilon$), then by far the fastest way to find the best model is with linear least squares regression. 

For example, if you wanted to fit the following polynomial model to data ($y$) measured at times ($t$)

$y(t) = \beta_0 + \beta_1 t+ \beta_2 t^2 + ... + \epsilon$

then you could express your model linearly where 

$$ \beta = [\beta_0, \beta_1, \beta_2, ...] \ \ \& \ \
    X = \begin{pmatrix}
    1 & t_1 & t_1^2 &  \\
    1 & t_2 & t_2^2 & \dots \\
    1 & t_3 & t_3^2 &  \\
     & \vdots &  & \ddots \\
    \end{pmatrix}
$$

If your data is has homoscedastic and uncorrelated Gaussian errors (i.e. the errors are the same across all values of the measured or independent variables and don't depend on each other) then you can use ordinary least squares to find the ["best"](https://en.wikipedia.org/wiki/UMVU) estimate for the model parameters

$\hat{\beta} = (X^T X)^{-1} X^T y$

[OLS](https://en.wikipedia.org/wiki/Ordinary_least_squares#Matrix/vector_formulation)

This is equivalent to finding the maximum likelihood estimate of the model parameters for Gaussian distributed data.

If your data has heteroscedastic and/or correlated Gaussian errors, then you have to use the (only slightly) more complicated generalized least squares which involves the data covariances

$\hat{\beta} = (X^T \Omega^{-1} X)^{-1} X^T \Omega^{-1} y$

where $\Omega$ is the covariance matrix of your data

[GLS](https://en.wikipedia.org/wiki/Generalized_least_squares#Method_outline)

```julia
σ = 0.2
measurements(t::Real; σ::Real=0.0) = σ * randn() + sin(t * 2 * π)
```

```julia
using Plots
gr(size=(711, 400)) # , leg=false)
```

```julia
# times = sort(rand(10))
# times = sort(collect(range(0, 1, step = 1/10)) .+ 0.03 .* randn(11))
times = [-0.029420531390630337, 0.05937118750818386, 0.2671315486134463, 0.3158992229761232, 0.4384690024307414, 0.5213304110953845, 0.6162234212074693, 0.6680339634409973, 0.778601173018435, 0.9307096597820922, 1.0047502472600456]
n = length(times)
# data = measurements.(times; σ=σ)
data = [-0.5129299697872882, 0.3249590031693929, 1.2034083920915803, 0.9342805146435638, 0.2907109653244365, -0.4342659514970483, -0.6918164600596547, -0.5128245589349643, -1.1556669153060266, -0.24729048035974605, -0.0639886128929851]
times_plot = collect(range(minimum(times) - .2, maximum(times) + .2, step = 1/100))
data_noiseless = measurements.(times_plot)
scatter(times, data, yerror=σ, label="Data")
plot!(times_plot, data_noiseless, label="")
```

```julia
OLS(X, y) = (X' * X) \ X' * y

max_polynomial_degree = 9

function build_X(times)
    n = length(times)
    X = zeros(n, max_polynomial_degree+1)
    X[:, 1] = ones(n)
    for j in 2:max_polynomial_degree+1
        X[:, j] = X[:, j - 1] .* times
    end
    return X
end

X = build_X(times)
X_plot = build_X(times_plot)

labels = ["Constant", "Linear", "Quadratic", "Cubic", "Quartic", "Quintic", "Sextic", "Septic", "Optic", "Nonic"]

function plot_linear_fit(degree::Integer)
    degree += 1
    plt = scatter(times, data, yerror=σ, label="Data")
    plot!(times_plot, data_noiseless, label="")
    β = OLS(X[:, 1:degree], data)
    model = X_plot[:, 1:degree] * β
    plot!(times_plot, model, lw=1, label=labels[degree])
    ylims!(-1.5,1.5)
    return plt
end

plot(plot_linear_fit(0), plot_linear_fit(1), plot_linear_fit(3), plot_linear_fit(9), layout = (2, 2))
```

which of these models is the best? higher orders fit more closely to the data but seem to be overcorrecting for fit noise

we can use regularization to penalize complexity

but we no longer have a linear model so have to resort to the much slower nonlinear optimization :(

Flux is a nonlinear model construction and fitting package for Julia that automatically calculates gradients to speed up fitting

```julia
using Flux
```

```julia
model_func(β, X) = X * β

function simple_train(poly_degree::Integer)
    X_temp = X[:, 1:poly_degree+1]
    β_flux = rand(poly_degree+1)
    
    loss(X, y; β=β_flux) = sum((y - model_func(β, X)) .^ 2)
        
    ps = Flux.params(β_flux)
    flux_data = Iterators.repeated((X_temp, data), Int(3e3 * poly_degree))
    opt = ADAM(1)

    Flux.train!(loss, ps, flux_data, opt)
end
```

nonlinear fitting is slower :(

```julia
using BenchmarkTools

@btime OLS(X, data);

@btime simple_train(9);
```

```julia
function train_with_regularization(poly_degree::Integer)
    @assert 0 <= poly_degree <= max_polynomial_degree
    X_temp = X[:, 1:poly_degree+1]
    λ = 0.01  # controls relative importance between data and penalty
    q = 1  # controls shape of penalty. q=1 is called LASSO regression or L1 regularization. q=2 is called ridge regression or L2 regularization
    penalty(β) = λ * sum(abs.(β) .^ q)
    loss(X, y; β=β_flux) = sum((y - model_func(β, X)) .^ 2) + penalty(β)
    
    β = OLS(X_temp, data)
    β_flux = rand(poly_degree + 1)
    
    ps = Flux.params(β_flux)
    flux_data = Iterators.repeated((X_temp, data), Int(3e3 * poly_degree))
    opt = ADAM(1)
    
    cb() = println("current loss: ", loss(X_temp, data))

    Flux.train!(loss, ps, flux_data, opt, cb = Flux.throttle(cb, 0.02 * poly_degree))
    
    println()
    println(β_flux)
    println("final loss: ", loss(X_temp, data))
    println()
    println(β)
    println("OLS loss: ", loss(X_temp, data; β=β))
    
    plot_linear_fit(poly_degree)
    plot!(times_plot, model_func(β_flux, X_plot[:, 1:poly_degree+1]), lw=1, label=labels[poly_degree+1] * " (Flux estimated)")
end
```

```julia
train_with_regularization(9)
```

```julia
train_with_regularization(6)
```

```julia
train_with_regularization(3)
```

```julia
train_with_regularization(1)
```

For the too complex models, regularization helped to make them more realistic and generalize slightly better!

# Switching gears

```julia
# https://github.com/FluxML/model-zoo/blob/master/other/iris/iris.jl
# https://github.com/FluxML/Flux.jl/blob/master/docs/src/models/regularisation.md
using Flux
using Flux: crossentropy, normalise, onecold, onehotbatch
using Statistics: mean
```

```julia
labels = Flux.Data.Iris.labels()
features = Flux.Data.Iris.features()
```

```julia
# Subract mean, divide by std dev for normed mean of 0 and std dev of 1.
normed_features = normalise(features, dims=2)


klasses = sort(unique(labels))
onehot_labels = onehotbatch(labels, klasses)


# Split into training and test sets, 2/3 for training, 1/3 for test.
train_indices = [1:3:150 ; 2:3:150]

X_train = normed_features[:, train_indices]
y_train = onehot_labels[:, train_indices]

X_test = normed_features[:, 3:3:150]
y_test = onehot_labels[:, 3:3:150]
```

```julia
# Declare model taking 4 features as inputs and outputting 3 probabiltiies, 
# one for each species of iris.
model = Chain(
    Dense(4, 3),
    softmax
)


loss(x, y) = crossentropy(model(x), y)

# Gradient descent optimiser with learning rate 0.5.
optimiser = Descent(0.5)

# Create iterator to train model over 110 epochs.
data_iterator = Iterators.repeated((X_train, y_train), 110)

println("Starting training.")
Flux.train!(loss, params(model), data_iterator, optimiser)
```

```julia
# Evaluate trained model against test set.
accuracy(x, y) = mean(onecold(model(x)) .== onecold(y))

accuracy_score = accuracy(X_test, y_test)

println("\nAccuracy: $accuracy_score")

# Sanity check.
@assert accuracy_score > 0.8
```

```julia
function confusion_matrix(X, y)
    ŷ = onehotbatch(onecold(model(X)), 1:3)
    y * ŷ'
end
#To avoid confusion, here is the definition of a Confusion Matrix: https://en.wikipedia.org/wiki/Confusion_matrix
println("\nConfusion Matrix:\n")
confusion_matrix(X_test, y_test)
```

# Notes
David Banks gives few hour overview of ML
give ML context in notebook

classifying stability of planetary systems?

make ROC curves?

show how optimizer is improving things as it works

show right answer with a linear problem

linear and logistic regression

regulatization terms?

## more specifically...

simple tutorial

show a problem that you dont need optimization to solve, but then solve it with optimization methods

compare iterative methods to analytical

what if we want a regulatization for sparseity/some breaking assumption

everyone will achieve something - have a bonus exploring thing at the end

hand holdy -> advanced (do the same problem with more dimensions to see scaling)

comparing algorithms

```julia
# https://towardsdatascience.com/regularization-an-important-concept-in-machine-learning-5891628907ea
# regularization example

# https://www.machinelearningplus.com/machine-learning/logistic-regression-tutorial-examples-r/
# logistic regression example
```
